{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Jane Howard\n",
        "\n",
        "Gen AI\n",
        "\n",
        "Assignment 4"
      ],
      "metadata": {
        "id": "HhBoIv24m0f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory Questions\n",
        "\n",
        "Q1: GANs are formulated as a two-player minimax game between a generator G and a discriminator D. In the minimax objective function, the discriminator D aims to maximize the probability of correctly classifying real samples as real and generated sample as fake. The generator G is trained to fool the discriminator by producing samples that the discriminator is not able to distiniguish from real data. The minimax structure ensures competitive training as imporvments in one of the networks directly challenges the other network. For example, as the discriminator becomes better at identifying the fake samples, the generator better learns to produce more realistic data to fool it.\n",
        "\n",
        "\n",
        "Q2: Mode collapse is a failure mode in GAN training in which the generator only produces a limited variety of outputs. Often the outputs repeat very similar or the same samples instead of capturing differences in the data distribution. Mode collapse can happen because the generator is trained solely to fool the discriminator rather than to fully capture the diversity of the realistic data. If a small set of outputs consistently fools the discriminator, the generator can repeatedly produce those outputs and ignore other data modes.\n",
        "\n",
        "There are many techniques that have been proposed to mitigate mode collapse such as batch normalization, minibatch discrimination, and WGAN. These techniques help to stabilize the training process and encourage variety in generated samples.\n",
        "\n",
        "\n",
        "Q3: In GAN training, the discriminator acts as a learned loss function for the generator. The discriminators primary function is to distinguish between real data  from the dataset and fake samples created by the generator. During training the discriminator provides gradients to the generator that indicate how realistic the generated samples are. If the discriminator is well trained, the gradients will be informative and guide the generator toward producing more realistic data. However, if the discriminator is weak the generator may produce low quality samples. Additionally, the discriminator can be too strong which could result in vanishing gradients for the generator.\n",
        "\n",
        "\n",
        "Q4: The Inception Score (IS) and Fréchet Inception Distance (FID) are commonly used metrics for evaluating the quality of GAN generated samples. The IS measures the quality and diversity of generated images by using a pretrained inception network. The higher quality images have more confident class predictions and the diverse outputs have a broder distribution of predicted classes. IS does not compare generated samples to real data which can be misleading.\n",
        "\n",
        "The FID compares statistical similarity between real and generated images in the feature space of a pretrained network. FID computes the distance between the mean and covariance of real and generated images. A lower FID score indicates that the generated images are closer to the real data distribution.\n"
      ],
      "metadata": {
        "id": "vkK8u55wm_eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# config\n",
        "EPOCHS = 50\n",
        "NOISE_DIM = 128\n",
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 50000\n",
        "SAVE_EVERY = 10\n",
        "OUTPUT_DIR = \"gan_cifar10_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# data CIFAR10\n",
        "(train_images, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "train_images = train_images.astype(\"float32\")\n",
        "train_images = (train_images - 127.5) / 127.5\n",
        "\n",
        "train_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(train_images)\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "5wDIVW_CnCkI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23e3f26-35a7-4389-ecf5-89ed78e0c088"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generator model\n",
        "def make_generator_model(noise_dim=NOISE_DIM):\n",
        "    # output: 32x32x3 with tanh\n",
        "    model = tf.keras.Sequential(name=\"generator\")\n",
        "\n",
        "    model.add(layers.Input(shape=(noise_dim,)))\n",
        "    model.add(layers.Dense(4 * 4 * 512, use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Reshape((4, 4, 512)))  #4x4x512\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())  #8x8x256\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())  #16x16x128\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (3, 3), strides=(1, 1), padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())  #16x16x128\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())  #32x32x64\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (3, 3), strides=(1, 1), padding=\"same\", use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())  #32x32x64\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(3, (3, 3), strides=(1, 1), padding=\"same\", use_bias=False, activation=\"tanh\"))\n",
        "    return model"
      ],
      "metadata": {
        "id": "IoEtZp38okmm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Discriminator model\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential(name=\"discriminator\")\n",
        "    model.add(layers.Input(shape=(32, 32, 3)))\n",
        "\n",
        "    model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (4, 4), strides=(2, 2), padding=\"same\"))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "    return model\n",
        "\n",
        "\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n"
      ],
      "metadata": {
        "id": "TMTcFufco3_5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#losses and optimizers\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_logits, fake_logits):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_logits), real_logits)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_logits), fake_logits)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(fake_logits):\n",
        "    return cross_entropy(tf.ones_like(fake_logits), fake_logits)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
        "\n",
        "num_examples_to_generate = 16\n",
        "seed = tf.random.normal([num_examples_to_generate, NOISE_DIM])\n",
        "\n",
        "# image saving\n",
        "def save_images(model, epoch, test_input, out_dir=OUTPUT_DIR):\n",
        "    predictions = model(test_input, training=False)\n",
        "    images = (predictions + 1.0) / 2.0\n",
        "    images = tf.clip_by_value(images, 0.0, 1.0).numpy()\n",
        "\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    for i in range(images.shape[0]):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    path = os.path.join(out_dir, f\"epoch_{epoch:03d}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=150)\n",
        "    plt.close(fig)"
      ],
      "metadata": {
        "id": "EFnGD0nHpFNj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "@tf.function\n",
        "def train_step(real_images):\n",
        "    batch_size = tf.shape(real_images)[0]\n",
        "    noise = tf.random.normal([batch_size, NOISE_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        fake_images = generator(noise, training=True)\n",
        "\n",
        "        real_logits = discriminator(real_images, training=True)\n",
        "        fake_logits = discriminator(fake_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_logits)\n",
        "        disc_loss = discriminator_loss(real_logits, fake_logits)\n",
        "\n",
        "    gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n"
      ],
      "metadata": {
        "id": "8jirDR-SpOdP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training loop\n",
        "def train(dataset, epochs):\n",
        "    save_images(generator, 0, seed)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for batch in dataset:\n",
        "            train_step(batch)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} completed\")\n",
        "\n",
        "        if epoch % SAVE_EVERY == 0:\n",
        "            save_images(generator, epoch, seed)\n"
      ],
      "metadata": {
        "id": "8gDJize3pS40"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)\n",
        "print(\"Done. Check gan_cifar10_outputs for images.\")\n"
      ],
      "metadata": {
        "id": "6SSTRmnerLdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8920eb-fbe9-4b9b-8bad-6d04b21b6980"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 completed\n",
            "Epoch 2/50 completed\n",
            "Epoch 3/50 completed\n",
            "Epoch 4/50 completed\n",
            "Epoch 5/50 completed\n",
            "Epoch 6/50 completed\n",
            "Epoch 7/50 completed\n",
            "Epoch 8/50 completed\n",
            "Epoch 9/50 completed\n",
            "Epoch 10/50 completed\n",
            "Epoch 11/50 completed\n",
            "Epoch 12/50 completed\n",
            "Epoch 13/50 completed\n",
            "Epoch 14/50 completed\n",
            "Epoch 15/50 completed\n",
            "Epoch 16/50 completed\n",
            "Epoch 17/50 completed\n",
            "Epoch 18/50 completed\n",
            "Epoch 19/50 completed\n",
            "Epoch 20/50 completed\n",
            "Epoch 21/50 completed\n",
            "Epoch 22/50 completed\n",
            "Epoch 23/50 completed\n",
            "Epoch 24/50 completed\n",
            "Epoch 25/50 completed\n",
            "Epoch 26/50 completed\n",
            "Epoch 27/50 completed\n",
            "Epoch 28/50 completed\n",
            "Epoch 29/50 completed\n",
            "Epoch 30/50 completed\n",
            "Epoch 31/50 completed\n",
            "Epoch 32/50 completed\n",
            "Epoch 33/50 completed\n",
            "Epoch 34/50 completed\n",
            "Epoch 35/50 completed\n",
            "Epoch 36/50 completed\n",
            "Epoch 37/50 completed\n",
            "Epoch 38/50 completed\n",
            "Epoch 39/50 completed\n",
            "Epoch 40/50 completed\n",
            "Epoch 41/50 completed\n",
            "Epoch 42/50 completed\n",
            "Epoch 43/50 completed\n",
            "Epoch 44/50 completed\n",
            "Epoch 45/50 completed\n",
            "Epoch 46/50 completed\n",
            "Epoch 47/50 completed\n",
            "Epoch 48/50 completed\n",
            "Epoch 49/50 completed\n",
            "Epoch 50/50 completed\n",
            "Done. Check gan_cifar10_outputs for images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bf1m1SMCGml5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}